{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of Pfruit.ipynb","provenance":[{"file_id":"1uM0ND9jsDK9QvO6zTM1dd5TlvVnivqU-","timestamp":1637966604769}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"papermill":{"default_parameters":{},"duration":764.028828,"end_time":"2021-08-19T13:31:16.403130","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-08-19T13:18:32.374302","version":"2.3.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"98173594"},"source":["## Passion Fruits, Object detection and localization model"],"id":"98173594"},{"cell_type":"code","metadata":{"id":"f0b5b215"},"source":["import cv2\n","import ast\n","\n","import numpy as np \n","import pandas as pd \n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.image as immg\n","\n","import random\n","\n","import torch\n","\n","import torchvision\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as T\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from tqdm.notebook import tqdm\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import math\n","import sys\n","import time\n","\n","import torch\n","import torchvision.models.detection.mask_rcnn"],"id":"f0b5b215","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbPyGRWhCAwe"},"source":["from google.colab import drive"],"id":"nbPyGRWhCAwe","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VHHkRYveCWw1","executionInfo":{"status":"ok","timestamp":1638413212488,"user_tz":300,"elapsed":35124,"user":{"displayName":"d f","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18133366172437363700"}},"outputId":"31f5f856-730b-4c57-b613-1afe5e163518"},"source":["drive.mount('/content/drive')"],"id":"VHHkRYveCWw1","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"794f2cc6"},"source":["### Read the csv files"],"id":"794f2cc6"},{"cell_type":"markdown","metadata":{"id":"e4938057"},"source":["### Train"],"id":"e4938057"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"3a8037e9","executionInfo":{"status":"ok","timestamp":1638413212771,"user_tz":300,"elapsed":161,"user":{"displayName":"d f","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18133366172437363700"}},"outputId":"e51fb7b2-fa35-45bd-8334-b39cbee592c0"},"source":["train_df = pd.read_csv('/content/drive/MyDrive/Data/PassionFruit/Train.csv')\n","train_df.head()"],"id":"3a8037e9","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Image_ID</th>\n","      <th>class</th>\n","      <th>xmin</th>\n","      <th>ymin</th>\n","      <th>width</th>\n","      <th>height</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ID_007FAIEI</td>\n","      <td>fruit_woodiness</td>\n","      <td>87.0</td>\n","      <td>87.5</td>\n","      <td>228.0</td>\n","      <td>311.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ID_00G8K1V3</td>\n","      <td>fruit_brownspot</td>\n","      <td>97.5</td>\n","      <td>17.5</td>\n","      <td>245.0</td>\n","      <td>354.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ID_00WROUT9</td>\n","      <td>fruit_brownspot</td>\n","      <td>156.5</td>\n","      <td>209.5</td>\n","      <td>248.0</td>\n","      <td>302.5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ID_00ZJEEK3</td>\n","      <td>fruit_healthy</td>\n","      <td>125.0</td>\n","      <td>193.0</td>\n","      <td>254.5</td>\n","      <td>217.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ID_018UIENR</td>\n","      <td>fruit_brownspot</td>\n","      <td>79.5</td>\n","      <td>232.5</td>\n","      <td>233.5</td>\n","      <td>182.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Image_ID            class   xmin   ymin  width  height\n","0  ID_007FAIEI  fruit_woodiness   87.0   87.5  228.0   311.0\n","1  ID_00G8K1V3  fruit_brownspot   97.5   17.5  245.0   354.5\n","2  ID_00WROUT9  fruit_brownspot  156.5  209.5  248.0   302.5\n","3  ID_00ZJEEK3    fruit_healthy  125.0  193.0  254.5   217.0\n","4  ID_018UIENR  fruit_brownspot   79.5  232.5  233.5   182.0"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"Oy9hUJpfBXao"},"source":[""],"id":"Oy9hUJpfBXao","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3a75677","executionInfo":{"status":"ok","timestamp":1638413212931,"user_tz":300,"elapsed":183,"user":{"displayName":"d f","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18133366172437363700"}},"outputId":"a67f6b4e-6307-48d0-a756-ff98bab76822"},"source":["train_df.shape"],"id":"e3a75677","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3906, 6)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3b04ea96","executionInfo":{"status":"ok","timestamp":1638413212934,"user_tz":300,"elapsed":17,"user":{"displayName":"d f","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18133366172437363700"}},"outputId":"49c29af7-b4f2-4bac-a55e-90cf28706443"},"source":["No_duplicates = train_df.drop_duplicates(subset=\"Image_ID\")\n","print(No_duplicates.shape)"],"id":"3b04ea96","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(3001, 6)\n"]}]},{"cell_type":"markdown","metadata":{"id":"eca12cf5"},"source":["We have 3906 bounding boxes, from a collection of 3001 images. Some images have 1 bounding box, while others have more than 1 bounding box."],"id":"eca12cf5"},{"cell_type":"markdown","metadata":{"id":"3997c08f"},"source":["### Test"],"id":"3997c08f"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"38221f72","executionInfo":{"status":"ok","timestamp":1638413213097,"user_tz":300,"elapsed":172,"user":{"displayName":"d f","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18133366172437363700"}},"outputId":"a58bf020-9de6-47a3-de8f-4cce27b0f5a7"},"source":["test_df = pd.read_csv(\"/content/drive/MyDrive/Data/PassionFruit/Test.csv\")\n","test_df.head()"],"id":"38221f72","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Image_ID</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ID_IUJJG62B</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ID_ZPNDRD4T</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ID_AHFYB64P</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ID_L8JZLNTF</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ID_IFMUXGPL</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Image_ID\n","0  ID_IUJJG62B\n","1  ID_ZPNDRD4T\n","2  ID_AHFYB64P\n","3  ID_L8JZLNTF\n","4  ID_IFMUXGPL"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ferAdkZHCF54","executionInfo":{"status":"ok","timestamp":1638413213099,"user_tz":300,"elapsed":26,"user":{"displayName":"d f","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18133366172437363700"}},"outputId":"339e83a3-1e1f-4dc3-a833-40902094e9af"},"source":["test_df.shape"],"id":"ferAdkZHCF54","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(931, 1)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f946fa0e","executionInfo":{"status":"ok","timestamp":1638413213100,"user_tz":300,"elapsed":22,"user":{"displayName":"d f","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18133366172437363700"}},"outputId":"d8bd2a97-2257-4cb6-a780-68f1c87f85e0"},"source":["test_df.shape"],"id":"f946fa0e","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(931, 1)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"f7c1c54b"},"source":["# train_df['xmax'] = train_df['xmin']+train_df1['width']\n","# train_df['ymax'] = train_df['ymin']+train_df['height']"],"id":"f7c1c54b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"599060cb"},"source":["# test_df = test_df.loc[:2]"],"id":"599060cb","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"afb7b46d"},"source":["- Encode the classes from str to int"],"id":"afb7b46d"},{"cell_type":"code","metadata":{"id":"877cae64"},"source":["classes_la = {\"fruit_brownspot\": 1, \"fruit_healthy\": 2, \"fruit_woodiness\":3}\n","\n","train_df[\"class\"] = train_df[\"class\"].apply(lambda x: classes_la[x])"],"id":"877cae64","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"89d5fb4d","executionInfo":{"status":"ok","timestamp":1638413213264,"user_tz":300,"elapsed":181,"user":{"displayName":"d f","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18133366172437363700"}},"outputId":"fd50a12a-163b-4245-deae-579785955911"},"source":["train_df.head()"],"id":"89d5fb4d","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Image_ID</th>\n","      <th>class</th>\n","      <th>xmin</th>\n","      <th>ymin</th>\n","      <th>width</th>\n","      <th>height</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ID_007FAIEI</td>\n","      <td>3</td>\n","      <td>87.0</td>\n","      <td>87.5</td>\n","      <td>228.0</td>\n","      <td>311.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ID_00G8K1V3</td>\n","      <td>1</td>\n","      <td>97.5</td>\n","      <td>17.5</td>\n","      <td>245.0</td>\n","      <td>354.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ID_00WROUT9</td>\n","      <td>1</td>\n","      <td>156.5</td>\n","      <td>209.5</td>\n","      <td>248.0</td>\n","      <td>302.5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ID_00ZJEEK3</td>\n","      <td>2</td>\n","      <td>125.0</td>\n","      <td>193.0</td>\n","      <td>254.5</td>\n","      <td>217.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ID_018UIENR</td>\n","      <td>1</td>\n","      <td>79.5</td>\n","      <td>232.5</td>\n","      <td>233.5</td>\n","      <td>182.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Image_ID  class   xmin   ymin  width  height\n","0  ID_007FAIEI      3   87.0   87.5  228.0   311.0\n","1  ID_00G8K1V3      1   97.5   17.5  245.0   354.5\n","2  ID_00WROUT9      1  156.5  209.5  248.0   302.5\n","3  ID_00ZJEEK3      2  125.0  193.0  254.5   217.0\n","4  ID_018UIENR      1   79.5  232.5  233.5   182.0"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"HvtvLVHfRTJG"},"source":["xmax = train_df['xmin']+train_df['width']\n","# train_df['ymax'] = train_df['ymin']+train_df['height']\n","train_df.insert(4, column='xmax', value=xmax)"],"id":"HvtvLVHfRTJG","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H446BgFxRU4A"},"source":["ymax = train_df['ymin']+train_df['height']\n","# train_df['ymax'] = train_df['ymin']+train_df['height']\n","train_df.insert(5, column='ymax', value=ymax)"],"id":"H446BgFxRU4A","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"iExVMJ6z7UoA","executionInfo":{"status":"ok","timestamp":1638413213269,"user_tz":300,"elapsed":23,"user":{"displayName":"d f","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18133366172437363700"}},"outputId":"d8d64e8e-93c1-4e4e-aaca-46b89d256a21"},"source":["# train_df = train_df.loc[:20]\n","train_df.head()"],"id":"iExVMJ6z7UoA","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Image_ID</th>\n","      <th>class</th>\n","      <th>xmin</th>\n","      <th>ymin</th>\n","      <th>xmax</th>\n","      <th>ymax</th>\n","      <th>width</th>\n","      <th>height</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ID_007FAIEI</td>\n","      <td>3</td>\n","      <td>87.0</td>\n","      <td>87.5</td>\n","      <td>315.0</td>\n","      <td>398.5</td>\n","      <td>228.0</td>\n","      <td>311.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ID_00G8K1V3</td>\n","      <td>1</td>\n","      <td>97.5</td>\n","      <td>17.5</td>\n","      <td>342.5</td>\n","      <td>372.0</td>\n","      <td>245.0</td>\n","      <td>354.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ID_00WROUT9</td>\n","      <td>1</td>\n","      <td>156.5</td>\n","      <td>209.5</td>\n","      <td>404.5</td>\n","      <td>512.0</td>\n","      <td>248.0</td>\n","      <td>302.5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ID_00ZJEEK3</td>\n","      <td>2</td>\n","      <td>125.0</td>\n","      <td>193.0</td>\n","      <td>379.5</td>\n","      <td>410.0</td>\n","      <td>254.5</td>\n","      <td>217.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ID_018UIENR</td>\n","      <td>1</td>\n","      <td>79.5</td>\n","      <td>232.5</td>\n","      <td>313.0</td>\n","      <td>414.5</td>\n","      <td>233.5</td>\n","      <td>182.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Image_ID  class   xmin   ymin   xmax   ymax  width  height\n","0  ID_007FAIEI      3   87.0   87.5  315.0  398.5  228.0   311.0\n","1  ID_00G8K1V3      1   97.5   17.5  342.5  372.0  245.0   354.5\n","2  ID_00WROUT9      1  156.5  209.5  404.5  512.0  248.0   302.5\n","3  ID_00ZJEEK3      2  125.0  193.0  379.5  410.0  254.5   217.0\n","4  ID_018UIENR      1   79.5  232.5  313.0  414.5  233.5   182.0"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"Gce-Gl7KblZ6"},"source":["# train_df['xmin'] = train_df['xmin'].astype(np.float)\n","# train_df['ymin'] = train_df['ymin'].astype(np.float)\n","# train_df['width'] = train_df['width'].astype(np.float)\n","# train_df['height'] = train_df['height'].astype(np.float)\n","# train_df['xmax'] = train_df['xmax'].astype(np.float)\n","# train_df['ymax'] = train_df['ymax'].astype(np.float)"],"id":"Gce-Gl7KblZ6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WImDpQQF1nM0","executionInfo":{"status":"ok","timestamp":1638413213502,"user_tz":300,"elapsed":253,"user":{"displayName":"d f","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18133366172437363700"}},"outputId":"b00f3039-9fab-43eb-ae2e-15690192e0d6"},"source":["train_df.dtypes\n"],"id":"WImDpQQF1nM0","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Image_ID     object\n","class         int64\n","xmin        float64\n","ymin        float64\n","xmax        float64\n","ymax        float64\n","width       float64\n","height      float64\n","dtype: object"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"873d25d9"},"source":["#@title Default title text\n","df = train_df.copy() # create a copy of the train df"],"id":"873d25d9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1fc48d2e"},"source":["path = '/content/drive/MyDrive/Data/PassionFruit/Train_Images/'\n","path2 = '/content/drive/MyDrive/Data/PassionFruit/Test_Images/'"],"id":"1fc48d2e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d5204976"},"source":["# group by all bounding boxes (bbox)\n","df_grp = df.groupby(['Image_ID'])"],"id":"d5204976","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fc4ddf8c"},"source":["def plot_image(image_name):\n","    image_group = df_grp.get_group(image_name)\n","    bbox = image_group.loc[:,['xmin', 'ymin', 'xmax', 'ymax']]\n","    img = immg.imread(path+name+'.jpg')\n","    fig,ax = plt.subplots(figsize=(18,10))\n","    ax.imshow(img,cmap='binary')\n","    for i in range(len(bbox)):\n","        box = bbox.iloc[i].values\n","        print(bbox)\n","        x,y,w,h = box[0], box[1], box[2]-box[0], box[3]-box[1]\n","        rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=1,edgecolor='r',facecolor='none',)\n","        ax.text(*box[:2], image_group[\"class\"].values, verticalalignment='top', color='white', fontsize=13, weight='bold')\n","        ax.add_patch(rect)\n","    plt.show()"],"id":"fc4ddf8c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":328},"id":"d616840b","executionInfo":{"status":"error","timestamp":1638413213902,"user_tz":300,"elapsed":404,"user":{"displayName":"d f","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18133366172437363700"}},"outputId":"e5087b58-2850-462c-aa2e-e0eb07917793"},"source":["name = df.Image_ID.unique()[0]\n","plot_image(name)"],"id":"d616840b","execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-0bfd0a3eec89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage_ID\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-24-283449f30312>\u001b[0m in \u001b[0;36mplot_image\u001b[0;34m(image_name)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimage_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_grp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xmin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ymin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'xmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ymax'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimmg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1462\u001b[0m             raise ValueError('Only know how to handle PNG; with Pillow '\n\u001b[1;32m   1463\u001b[0m                              'installed, Matplotlib can handle more images')\n\u001b[0;32m-> 1464\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_png\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Data/PassionFruit/Train_Images/ID_007FAIEI.jpg'"]}]},{"cell_type":"code","metadata":{"id":"21ab1f84"},"source":["name = df.Image_ID.unique()[700]\n","# name = df.Image_ID.unique()[9]\n","plot_image(name)"],"id":"21ab1f84","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fd147b11"},"source":["name = df.Image_ID.unique()[7]\n","plot_image(name)"],"id":"fd147b11","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hyFYY-0y02E7"},"source":[""],"id":"hyFYY-0y02E7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ca682349"},"source":["class PassionFruit(object):\n","    def __init__(self, df, IMG_DIR, transforms=None): \n","        self.df = df\n","        self.img_dir = IMG_DIR\n","        self.image_ids = self.df['Image_ID'].unique()\n","        self.transforms = transforms\n","        \n","  \n","        self.classes = [\"fruit_brownspot\", \"fruit_healthy\", \"fruit_woodiness\"]\n","        # if self.classes[0] != \"background\":\n","        #     self.classes = [\"background\"] + self.classes\n","        # self.label_map = {c: i for i, c in enumerate(self.classes)}\n","        # self.lst = list(self.label_map.values())\n","        #self.label_map['background'] = 0\n","      \n","    def get_classes(self):\n","        return self.classes\n","\n","    def __len__(self):\n","        return len(self.image_ids)\n","    \n","    def __getitem__(self, idx):\n","        \n","        image_id = self.image_ids[idx]\n","        image_values = self.df[self.df['Image_ID'] == image_id]\n","        image = cv2.imread(self.img_dir+image_id+\".jpg\",cv2.IMREAD_COLOR)\n","        #img_path = os.parth.join()\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","        \n","        boxes = image_values[['xmin', 'ymin', 'xmax', 'ymax']].values\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        \n","        labels = image_values[\"class\"].values # \n","        labels = labels - 1\n","        labels = torch.tensor(labels)\n","        \n","        target = {}\n","        target['boxes'] = boxes\n","        target['labels'] = labels\n","        target['image_id'] = torch.tensor([idx])\n","        target['area'] = torch.as_tensor(area, dtype=torch.float32)\n","        target['iscrowd'] = torch.zeros(image_values.shape[0], dtype=torch.int64)\n","\n","        if self.transforms:\n","            sample = {\n","                'image': image,\n","                'bboxes': target['boxes'],\n","                'labels': labels\n","            }\n","        \n","            sample = self.transforms(**sample)\n","            image = sample['image']\n","            \n","            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n","\n","        return image, target"],"id":"ca682349","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rC8a2oZaDe5d"},"source":["!pip install albumentations==0.4.6"],"id":"rC8a2oZaDe5d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bc0ade9"},"source":["# !pip install -q -U albumentations\n","# !echo \"$(pip freeze | grep albumentations) is successfully installed\"\n","# import albumentations\n","# from albumentations.pytorch.transforms import ToTensorV2\n","# import albumentations as A\n","# !pip install torch\n","# !pip install torchvision\n","# !apt install -y albumentations \n","# !pip install -qU albumentations\n","\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","def get_train_transform():\n","    return A.Compose([\n","        A.Flip(0.5),\n","        #A.Resize(256, 256),\n","        ToTensorV2(p=1.0)\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","\n","def get_valid_transform():\n","    return A.Compose([\n","        #A.Resize(256, 256),\n","        ToTensorV2(p=1.0)\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"],"id":"5bc0ade9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5KLpZpTA8f9h"},"source":["import torchvision.transforms as T\n","def get_transforms(train):\n","  transforms = []\n","  transforms.append(T.ToTensor())\n","  if train:\n","    transforms.append(T.RandomHorizontalFlip(0.5))\n","  return T.Compose(transforms)"],"id":"5KLpZpTA8f9h","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1b9eb79e"},"source":["image_ids = df['Image_ID'].unique()\n","valid_ids = image_ids[-665:]\n","train_ids = image_ids[:-665]\n","valid_df = df[df['Image_ID'].isin(valid_ids)]\n","train_df = df[df['Image_ID'].isin(train_ids)]\n","train_df.shape,valid_df.shape"],"id":"1b9eb79e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HanJfOqbKWby"},"source":["\n","train_dataset = PassionFruit(train_df, path, get_train_transform())\n","valid_dataset = PassionFruit(valid_df, path, get_valid_transform())\n"],"id":"HanJfOqbKWby","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cF26ajP75yXL"},"source":["image, target = train_dataset.__getitem__(0)\n","print(image.shape)\n","print(target['boxes'])\n","print(target['labels'])"],"id":"cF26ajP75yXL","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"91685379"},"source":["img, tar = train_dataset[random.randint(0,10)]\n","bbox = tar['boxes']\n","fig,ax = plt.subplots(figsize=(18,10))\n","ax.imshow(img.permute(1,2,0).cpu().numpy())\n","for l in tar[\"labels\"].tolist():\n","    classes_la = {0:\"fruit_brownspot\",  1:\"fruit_healthy\", 2:\"fruit_woodiness\"}\n","    l = classes_la[l]\n","    for i in range(len(bbox)):\n","        box = bbox[i]\n","        x,y,w,h = box[0], box[1], box[2]-box[0], box[3]-box[1]\n","        rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=2,edgecolor='r',facecolor='none',)\n","        ax.text(*box[:2], l, verticalalignment='top', color='red', fontsize=13, weight='bold')\n","        ax.add_patch(rect)\n","    plt.show()"],"id":"91685379","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"393c2dfc"},"source":["#### Plotting the bounding boxes using the dataframe"],"id":"393c2dfc"},{"cell_type":"markdown","metadata":{"id":"zqlv8TlcOHrw"},"source":["# New Section"],"id":"zqlv8TlcOHrw"},{"cell_type":"code","metadata":{"id":"c0e99511"},"source":["def plot_image(image_name):\n","    image_group = df_grp.get_group(image_name)\n","    bbox = image_group.loc[:,['xmin', 'ymin', 'xmax', 'ymax']]\n","    img = immg.imread(path+name+'.jpg')\n","    fig,ax = plt.subplots(figsize=(18,10))\n","    ax.imshow(img,cmap='binary')\n","    for i in range(len(bbox)):\n","        box = bbox.iloc[i].values\n","        print(bbox)\n","        x,y,w,h = box[0], box[1], box[2]-box[0], box[3]-box[1]\n","        rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=1,edgecolor='r',facecolor='none',)\n","        ax.text(*box[:2], image_group[\"class\"].values, verticalalignment='top', color='white', fontsize=13, weight='bold')\n","        ax.add_patch(rect)\n","    plt.show()"],"id":"c0e99511","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yDRZHuyc3Zyd"},"source":[""],"id":"yDRZHuyc3Zyd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uzCJpH7lt3ge"},"source":["def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","train_data_loader = DataLoader(\n","  train_dataset,\n","  batch_size=4,\n","  shuffle=False,\n","  num_workers=4,\n","  collate_fn=collate_fn\n",")\n","\n","valid_data_loader = DataLoader(\n","  valid_dataset,\n","  batch_size=4,\n","  shuffle=False,\n","  num_workers=4,\n","  collate_fn=collate_fn\n",")"],"id":"uzCJpH7lt3ge","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ISkN7-bwUZS4"},"source":["def get_model(num_classes):\n","  model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\n","  in_features = model.roi_heads.box_predictor.cls_score.in_features\n","  model.roi_heads.box_predictor = KeypointRCNNPredictor(in_features, num_classes)\n","  return model"],"id":"ISkN7-bwUZS4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u_XtTiYNaV26"},"source":["!git clone https://github.com/pytorch/vision.git\n","!cd vision && git checkout v0.3.0\n","!cp vision/references/detection/utils.py ./\n","!cp vision/references/detection/transforms.py ./\n","!cp vision/references/detection/coco_eval.py ./\n","!cp vision/references/detection/engine.py ./\n","!cp vision/references/detection/coco_utils.py ./"],"id":"u_XtTiYNaV26","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YjhyrwKZYIL4"},"source":["from engine import train_one_epoch, evaluate\n","from utils import *\n","\n","def do_training(model, torch_dataset, torch_dataset_test, num_epochs=4):\n","    # define training and validation data loaders\n","    data_loader = torch.utils.data.DataLoader(\n","        torch_dataset, batch_size=4, shuffle=True, num_workers=4,\n","        collate_fn=collate_fn)\n","    \n","    data_loader_test = torch.utils.data.DataLoader(\n","        torch_dataset, batch_size=4, shuffle=False, num_workers=4,\n","        collate_fn=collate_fn)\n","\n","    # train on the GPU or on the CPU, if a GPU is not available\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    #device = torch.device(\"cpu\")\n","    print(\"Using device %s\" % device)\n","\n","    # move model to the right device\n","    model.to(device)\n","\n","    # construct an optimizer\n","    params = [p for p in model.parameters() if p.requires_grad]\n","    optimizer = torch.optim.SGD(params, lr=0.005,\n","                                momentum=0.9, weight_decay=0.0005)\n","    # and a learning rate scheduler\n","    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                                    step_size=3,\n","                                                    gamma=0.1)\n","\n","    for epoch in range(num_epochs):\n","        # train for one epoch, printing every 10 iterations\n","        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","\n","        # update the learning rate\n","        lr_scheduler.step()\n","        # evaluate on the test dataset\n","        evaluate(model, data_loader_test, device=device)\n","\n"],"id":"YjhyrwKZYIL4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YGi8S7N-TdfP"},"source":["\n","from torchvision.models.detection.keypoint_rcnn import KeypointRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","classes = [\"fruit_brownspot\", \"fruit_healthy\", \"fruit_woodiness\"]\n","model = get_model(len(classes)+ 1)\n"],"id":"YGi8S7N-TdfP","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1nr-tR6bYO6-"},"source":["do_training(model, train_dataset, valid_dataset, num_epochs=2)"],"id":"1nr-tR6bYO6-","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsQutogPX4Oy"},"source":["torch.save(model.state_dict(), '/content/drive/MyDrive/Data/PassionFruit/passion_fruit_Alexnet_model.pt')\n","from google.colab import files\n","files.download('passion_fruit_model.pt')"],"id":"LsQutogPX4Oy","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4JFQQ-of9c1"},"source":["from google.colab import files\n","files.download('passion_fruit_model.pt')"],"id":"C4JFQQ-of9c1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-0vZVyFKGtkg"},"source":["model.load_state_dict(torch.load('/content/drive/MyDrive/Data/PassionFruit/passion_fruit_model.pt', map_location=torch.device('cpu')))"],"id":"-0vZVyFKGtkg","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qw26-q-C1r6g"},"source":["from torchvision.transforms import functional as FF\n","\n","class TestPassionFruit(object):\n","    def __init__(self, df, IMG_DIR,transforms=None): \n","        self.df = df\n","        self.img_dir = IMG_DIR\n","        self.image_ids = self.df['Image_ID'].unique()\n","        self.transforms = transforms\n","        \n","  \n","        self.classes = [\"fruit_brownspot\", \"fruit_healthy\", \"fruit_woodiness\"]\n","        # if self.classes[0] != \"background\":\n","        #     self.classes = [\"background\"] + self.classes\n","        # self.label_map = {c: i for i, c in enumerate(self.classes)}\n","        # self.lst = list(label_map.values())\n","        #self.label_map['background'] = 0\n","      \n","    def get_classes(self):\n","        return self.classes\n","\n","    def __len__(self):\n","        return len(self.image_ids)\n","    \n","    def __getitem__(self, idx):\n","     \n","        image_id = self.image_ids[idx]\n","        image_values = self.df[self.df['Image_ID'] == image_id]\n","        image =  cv2.imread(self.img_dir+image_id+\".jpg\")\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image /= 255.0\n","        \n","        \n","\n","        if self.transforms:\n","            sample = {\n","                'image': image\n","            }\n","        \n","            sample = self.transforms(sample)\n","            image = sample['image']\n","\n","        return FF.to_tensor(image), image_id"],"id":"qw26-q-C1r6g","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PSMkCovH1yyA"},"source":["test_dataset = TestPassionFruit(test_df, path2)"],"id":"PSMkCovH1yyA","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ctFz-VFiQunw"},"source":["from torchvision import transforms\n","\n","def apply_nms(orig, iou_thresh=0.5):\n","  keep = torchvision.ops.nms(orig[0]['boxes'], orig[0]['scores'], iou_thresh)\n","  final = orig[0]\n","  final['boxes'] = final['boxes'][keep]\n","  final['scores'] = final['scores'][keep]\n","  final['labels'] = final['labels'][keep]\n","\n","  return final\n","\n","def torch_to_pil(img):\n","  return transforms.ToPILImage()(img).convert('RGB')"],"id":"ctFz-VFiQunw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WbuaCNulisGy"},"source":["# from torchvision import transforms\n","# aa = transforms.ToTensor()\n","# images, targets, id = next(iter(valid_dataset))\n","# images = list(img.unsqueeze(0) for img in images)\n","# print(targets)\n","# print(id)\n","# # targets = [{k: v for k, v in t.items()} for t in targets]\n","# # boxes = targets[1]['boxes'].cpu().numpy().astype(np.int32)\n","# sample = images[1].permute(1,2,0).cpu().numpy()\n"],"id":"WbuaCNulisGy","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CW_vinukFKXq"},"source":["img = test_dataset[0]\n","print(img[1])\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img[0]])"],"id":"CW_vinukFKXq","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lZTlUuanFk1n"},"source":["prediction"],"id":"lZTlUuanFk1n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOGyc2YSpaJq"},"source":["def plot_img_bbox(img, target):\n","    # plot the image and bboxes\n","    # Bounding boxes are defined as follows: x-min y-min width height\n","    fig, a = plt.subplots(1,1)\n","    fig.set_size_inches(5,5)\n","    a.imshow(img)\n","    for box in (target['boxes']):\n","        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n","        rect = patches.Rectangle((x, y),\n","                                 width, height,\n","                                 linewidth = 2,\n","                                 edgecolor = 'r',\n","                                 facecolor = 'none')\n","\n","        # Draw the bounding box on top of the image\n","        a.add_patch(rect)\n","    plt.show()"],"id":"bOGyc2YSpaJq","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gBlBdGmcmRUB"},"source":["ypred = apply_nms(prediction, 0.10)\n","ypred"],"id":"gBlBdGmcmRUB","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfHOUmGtmRic"},"source":["import matplotlib.patches as patches\n","plot_img_bbox(torch_to_pil(img[0]), ypred)"],"id":"cfHOUmGtmRic","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fYUJXyWlK8Yo"},"source":["# all_Ids = []\n","# bboxes = []\n","# all_labels = []\n","# xmin, ymin, xmax, xmin = [], [], [], []\n","# for img in test_dataset:\n","#   Ids = img[1]\n","#   all_Ids.append(Ids)\n","#   model.eval()\n","#   with torch.no_grad():\n","#     prediction = model([img[0]])\n","\n","#     ypred = apply_nms(prediction, 0.5)\n","#     xmin, ymin, xmax, xmax = [], [], [], []\n","   \n","#     for yp in range(len(ypred['boxes'])):\n","#       xmin.append(np.array([str(outputs['boxes'][i][0].item()))\n","#       ymin.append(np.array([str(outputs['boxes'][i][1].item()))\n","#       xmax.append(np.array([str(outputs['boxes'][i][2].item()))\n","#       xmax.append(np.array([str(outputs['boxes'][i][3].item()))\n","\n","#     for lab in range(len(ypred['labels'])):\n","#       all_labels.append(np.array([str(outputs['labels'][i].item()))\n","\n","#     for score in range(len(ypred['scores'])):\n","#       all_labels.append(np.array([str(outputs['scores'][i].item()))\n","\n","\n","\n","\n","\n"],"id":"fYUJXyWlK8Yo","execution_count":null,"outputs":[]}]}